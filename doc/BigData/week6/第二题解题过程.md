
第二题解答：

DistCp代码：
[DistCp.scala](spark-demo/src/main/java/DistCp.scala)


查看week6目录：
```shell
[student5@emr-header-1 haibo.duan]$ hadoop fs -ls /haibo.duan/week6/
Found 2 items
-rw-r-----   2 student5 hadoop         67 2022-06-12 17:11 /haibo.duan/week6/index.csv
-rw-r-----   2 student5 hadoop         56 2022-06-12 16:56 /haibo.duan/week6/index.txt
[student5@emr-header-1 haibo.duan]$
```
创建copytest目录：
```shell
[student5@emr-header-1 haibo.duan]$ hadoop fs -mkdir -p /haibo.duan/copytest/
[student5@emr-header-1 haibo.duan]$ hadoop fs -ls /haibo.duan
Found 5 items
-rw-r-----   2 student5 hadoop          0 2022-05-13 17:11 /haibo.duan/_SUCCESS
drwxr-x--x   - student5 hadoop          0 2022-06-12 18:19 /haibo.duan/copytest
-rw-r-----   2 student5 hadoop        551 2022-05-13 17:11 /haibo.duan/part-r-00000
drwxr-x--x   - student5 hadoop          0 2022-05-29 18:29 /haibo.duan/week4
drwxr-x--x   - student5 hadoop          0 2022-06-12 17:11 /haibo.duan/week6
```
spark 执行过程：
```shell
[student5@emr-header-1 haibo.duan]$ spark-submit --class Distcp --master local[*] /home/student5/haibo.duan/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar -i -m 3 "/haibo.duan/week6/" "/haibo.duan/copytest/"
Error: Failed to load class Distcp.
22/06/12 18:20:11 INFO [shutdown-hook-0] ShutdownHookManager: Shutdown hook called
22/06/12 18:20:11 INFO [shutdown-hook-0] ShutdownHookManager: Deleting directory /tmp/spark-aa52635e-7179-40b1-b9a0-c79a7be78c59
[student5@emr-header-1 haibo.duan]$ spark-submit --class DistCp --master local[*] /home/student5/haibo.duan/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar -i -m 3 "/haibo.duan/week6/" "/haibo.duan/copytest/"
Map('ignoreFailure -> 1, 'maxconcurrency -> 3, 'infile -> /haibo.duan/week6/, 'outfile -> /haibo.duan/copytest/)
22/06/12 18:20:27 INFO [main] SparkContext: Running Spark version 3.2.0
22/06/12 18:20:27 INFO [main] ResourceUtils: ==============================================================
22/06/12 18:20:27 INFO [main] ResourceUtils: No custom resources configured for spark.driver.
22/06/12 18:20:27 INFO [main] ResourceUtils: ==============================================================
22/06/12 18:20:27 INFO [main] SparkContext: Submitted application: bingbing
22/06/12 18:20:28 INFO [main] ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2145, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/06/12 18:20:28 INFO [main] ResourceProfile: Limiting resource is cpus at 1 tasks per executor
22/06/12 18:20:28 INFO [main] ResourceProfileManager: Added ResourceProfile id: 0
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
22/06/12 18:20:28 INFO [main] SecurityManager: Changing view acls to: student5,*
22/06/12 18:20:28 INFO [main] SecurityManager: Changing modify acls to: student5
22/06/12 18:20:28 INFO [main] SecurityManager: Changing view acls groups to: 
22/06/12 18:20:28 INFO [main] SecurityManager: Changing modify acls groups to: 
22/06/12 18:20:28 INFO [main] SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(student5, *); groups with view permissions: Set(); users  with modify permissions: Set(student5); groups with modify permissions: Set()
22/06/12 18:20:28 INFO [main] Utils: Successfully started service 'sparkDriver' on port 40399.
22/06/12 18:20:28 INFO [main] SparkEnv: Registering MapOutputTracker
22/06/12 18:20:28 INFO [main] SparkEnv: Registering BlockManagerMaster
22/06/12 18:20:28 INFO [main] BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/12 18:20:28 INFO [main] BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/12 18:20:28 INFO [main] SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/12 18:20:28 INFO [main] DiskBlockManager: Created local directory at /tmp/blockmgr-2fb27dbb-d33f-4365-8e15-ab8863ddbb65
22/06/12 18:20:28 INFO [main] MemoryStore: MemoryStore started with capacity 912.3 MiB
22/06/12 18:20:28 INFO [main] SparkEnv: Registering OutputCommitCoordinator
22/06/12 18:20:28 INFO [main] log: Logging initialized @1808ms to org.sparkproject.jetty.util.log.Slf4jLog
22/06/12 18:20:28 INFO [main] Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_252-b09
22/06/12 18:20:28 INFO [main] Server: Started @1881ms
22/06/12 18:20:28 WARN [main] Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/06/12 18:20:28 INFO [main] AbstractConnector: Started ServerConnector@1ef173c5{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
22/06/12 18:20:28 INFO [main] Utils: Successfully started service 'SparkUI' on port 4041.
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@2d10e0b1{/jobs,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6c345c5f{/jobs/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@65e61854{/jobs/job,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6f80fafe{/jobs/job/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@f9879ac{/stages,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@5f4d427e{/stages/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@224b4d61{/stages/stage,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@362a019c{/stages/stage/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/stages/pool,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@674c583e{/stages/pool/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/storage,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/storage/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@61861a29{/storage/rdd,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/storage/rdd/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/environment,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/executors,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/executors/threadDump,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/executors/threadDump/json,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@72ccd81a{/static,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@7859e786{/,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@314b8f2d{/api,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@4a3e3e8b{/jobs/job/kill,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@71104a4{/stages/stage/kill,null,AVAILABLE,@Spark}
22/06/12 18:20:28 INFO [main] SparkUI: Bound SparkUI to 0.0.0.0, and started at http://emr-header-1.cluster-285604:4041
22/06/12 18:20:28 INFO [main] SparkContext: Added JAR file:/home/student5/haibo.duan/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://emr-header-1.cluster-285604:40399/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1655029227867
22/06/12 18:20:28 INFO [main] Executor: Starting executor ID driver on host emr-header-1.cluster-285604
22/06/12 18:20:28 INFO [main] Executor: Fetching spark://emr-header-1.cluster-285604:40399/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1655029227867
22/06/12 18:20:28 INFO [main] TransportClientFactory: Successfully created connection to emr-header-1.cluster-285604/192.168.0.197:40399 after 18 ms (0 ms spent in bootstraps)
22/06/12 18:20:28 INFO [main] Utils: Fetching spark://emr-header-1.cluster-285604:40399/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar to /tmp/spark-b420eb80-71b9-4006-933f-196ccae23c9d/userFiles-0d9a84f4-2010-445e-823d-62bd4452215c/fetchFileTemp1080844647691323670.tmp
22/06/12 18:20:29 INFO [main] Executor: Adding file:/tmp/spark-b420eb80-71b9-4006-933f-196ccae23c9d/userFiles-0d9a84f4-2010-445e-823d-62bd4452215c/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar to class loader
22/06/12 18:20:29 INFO [main] Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43155.
22/06/12 18:20:29 INFO [main] NettyBlockTransferService: Server created on emr-header-1.cluster-285604:43155
22/06/12 18:20:29 INFO [main] BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/12 18:20:29 INFO [main] BlockManagerMaster: Registering BlockManager BlockManagerId(driver, emr-header-1.cluster-285604, 43155, None)
22/06/12 18:20:29 INFO [dispatcher-BlockManagerMaster] BlockManagerMasterEndpoint: Registering block manager emr-header-1.cluster-285604:43155 with 912.3 MiB RAM, BlockManagerId(driver, emr-header-1.cluster-285604, 43155, None)
22/06/12 18:20:29 INFO [main] BlockManagerMaster: Registered BlockManager BlockManagerId(driver, emr-header-1.cluster-285604, 43155, None)
22/06/12 18:20:29 INFO [main] BlockManager: external shuffle service port = 7337
22/06/12 18:20:29 INFO [main] BlockManager: Initialized BlockManager: BlockManagerId(driver, emr-header-1.cluster-285604, 43155, None)
22/06/12 18:20:29 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@1e7f2e0f{/metrics/json,null,AVAILABLE,@Spark}
22/06/12 18:20:29 INFO [main] SingleEventLogFileWriter: Logging events to hdfs://emr-header-1.cluster-285604:9000/spark-history/local-1655029228854.inprogress
22/06/12 18:20:30 INFO [Thread-23] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:30 INFO [main] MemoryStore: Block broadcast_0 stored as values in memory (estimated size 425.8 KiB, free 911.9 MiB)
22/06/12 18:20:30 INFO [main] MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 911.8 MiB)
22/06/12 18:20:30 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_0_piece0 in memory on emr-header-1.cluster-285604:43155 (size: 44.4 KiB, free: 912.3 MiB)
22/06/12 18:20:30 INFO [main] SparkContext: Created broadcast 0 from textFile at DistCp.scala:56
22/06/12 18:20:30 INFO [main] deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
22/06/12 18:20:30 INFO [main] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:30 INFO [main] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:30 INFO [main] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:30 INFO [main] GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
22/06/12 18:20:30 INFO [main] LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]
22/06/12 18:20:30 INFO [main] FileInputFormat: Total input files to process : 1
22/06/12 18:20:30 INFO [main] SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 3 output partitions
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:83)
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Parents of final stage: List()
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Missing parents: List()
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at saveAsTextFile at DistCp.scala:56), which has no missing parents
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_1 stored as values in memory (estimated size 123.6 KiB, free 911.7 MiB)
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 48.6 KiB, free 911.7 MiB)
22/06/12 18:20:30 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_1_piece0 in memory on emr-header-1.cluster-285604:43155 (size: 48.6 KiB, free: 912.2 MiB)
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at saveAsTextFile at DistCp.scala:56) (first 15 tasks are for partitions Vector(0, 1, 2))
22/06/12 18:20:30 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Adding task set 0.0 with 3 tasks resource profile 0
22/06/12 18:20:30 INFO [dispatcher-event-loop-0] TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (emr-header-1.cluster-285604, executor driver, partition 0, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:30 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.csv:0+22
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Thread-29] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] FileOutputCommitter: Saved output of task 'attempt_202206121820308304774129875077200_0002_m_000000_0' to hdfs://localhost:9000/haibo.duan/copytest/index.csv/_temporary/0/task_202206121820308304774129875077200_0002_m_000000
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] SparkHadoopMapRedUtil: attempt_202206121820308304774129875077200_0002_m_000000_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] Executor: Finished task 0.0 in stage 0.0 (TID 0). 1287 bytes result sent to driver
22/06/12 18:20:31 INFO [dispatcher-event-loop-1] TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (emr-header-1.cluster-285604, executor driver, partition 1, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] Executor: Running task 1.0 in stage 0.0 (TID 1)
22/06/12 18:20:31 INFO [task-result-getter-0] TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 433 ms on emr-header-1.cluster-285604 (executor driver) (1/3)
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.csv:22+22
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Thread-33] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] FileOutputCommitter: Saved output of task 'attempt_202206121820308304774129875077200_0002_m_000001_0' to hdfs://localhost:9000/haibo.duan/copytest/index.csv/_temporary/0/task_202206121820308304774129875077200_0002_m_000001
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] SparkHadoopMapRedUtil: attempt_202206121820308304774129875077200_0002_m_000001_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] Executor: Finished task 1.0 in stage 0.0 (TID 1). 1244 bytes result sent to driver
22/06/12 18:20:31 INFO [dispatcher-event-loop-0] TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (emr-header-1.cluster-285604, executor driver, partition 2, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] Executor: Running task 2.0 in stage 0.0 (TID 2)
22/06/12 18:20:31 INFO [task-result-getter-1] TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 44 ms on emr-header-1.cluster-285604 (executor driver) (2/3)
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.csv:44+23
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Thread-35] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] FileOutputCommitter: Saved output of task 'attempt_202206121820308304774129875077200_0002_m_000002_0' to hdfs://localhost:9000/haibo.duan/copytest/index.csv/_temporary/0/task_202206121820308304774129875077200_0002_m_000002
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] SparkHadoopMapRedUtil: attempt_202206121820308304774129875077200_0002_m_000002_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] Executor: Finished task 2.0 in stage 0.0 (TID 2). 1244 bytes result sent to driver
22/06/12 18:20:31 INFO [task-result-getter-2] TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 38 ms on emr-header-1.cluster-285604 (executor driver) (3/3)
22/06/12 18:20:31 INFO [task-result-getter-2] TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:83) finished in 0.649 s
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/06/12 18:20:31 INFO [main] DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 0.719351 s
22/06/12 18:20:31 INFO [main] SparkHadoopWriter: Start to commit write Job job_202206121820308304774129875077200_0002.
22/06/12 18:20:31 INFO [main] SparkHadoopWriter: Write Job job_202206121820308304774129875077200_0002 committed. Elapsed time: 19 ms.
22/06/12 18:20:31 INFO [main] MemoryStore: Block broadcast_2 stored as values in memory (estimated size 425.8 KiB, free 911.3 MiB)
22/06/12 18:20:31 INFO [main] MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 911.2 MiB)
22/06/12 18:20:31 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_2_piece0 in memory on emr-header-1.cluster-285604:43155 (size: 44.4 KiB, free: 912.2 MiB)
22/06/12 18:20:31 INFO [main] SparkContext: Created broadcast 2 from textFile at DistCp.scala:56
22/06/12 18:20:31 INFO [main] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [main] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [main] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [main] FileInputFormat: Total input files to process : 1
22/06/12 18:20:31 INFO [main] SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 4 output partitions
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Parents of final stage: List()
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Missing parents: List()
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at DistCp.scala:56), which has no missing parents
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_3 stored as values in memory (estimated size 123.6 KiB, free 911.1 MiB)
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 48.6 KiB, free 911.0 MiB)
22/06/12 18:20:31 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_3_piece0 in memory on emr-header-1.cluster-285604:43155 (size: 48.6 KiB, free: 912.1 MiB)
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1427
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at DistCp.scala:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0
22/06/12 18:20:31 INFO [dispatcher-event-loop-0] TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (emr-header-1.cluster-285604, executor driver, partition 0, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] Executor: Running task 0.0 in stage 1.0 (TID 3)
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.txt:0+18
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Thread-38] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] FileOutputCommitter: Saved output of task 'attempt_20220612182031286352343316660483_0005_m_000000_0' to hdfs://localhost:9000/haibo.duan/copytest/index.txt/_temporary/0/task_20220612182031286352343316660483_0005_m_000000
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] SparkHadoopMapRedUtil: attempt_20220612182031286352343316660483_0005_m_000000_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 3)] Executor: Finished task 0.0 in stage 1.0 (TID 3). 1244 bytes result sent to driver
22/06/12 18:20:31 INFO [dispatcher-event-loop-1] TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (emr-header-1.cluster-285604, executor driver, partition 1, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] Executor: Running task 1.0 in stage 1.0 (TID 4)
22/06/12 18:20:31 INFO [task-result-getter-3] TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 47 ms on emr-header-1.cluster-285604 (executor driver) (1/4)
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.txt:18+18
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Thread-40] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] FileOutputCommitter: Saved output of task 'attempt_20220612182031286352343316660483_0005_m_000001_0' to hdfs://localhost:9000/haibo.duan/copytest/index.txt/_temporary/0/task_20220612182031286352343316660483_0005_m_000001
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] SparkHadoopMapRedUtil: attempt_20220612182031286352343316660483_0005_m_000001_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 1.0 in stage 1.0 (TID 4)] Executor: Finished task 1.0 in stage 1.0 (TID 4). 1244 bytes result sent to driver
22/06/12 18:20:31 INFO [dispatcher-event-loop-0] TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5) (emr-header-1.cluster-285604, executor driver, partition 2, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] Executor: Running task 2.0 in stage 1.0 (TID 5)
22/06/12 18:20:31 INFO [task-result-getter-0] TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 35 ms on emr-header-1.cluster-285604 (executor driver) (2/4)
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.txt:36+18
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Thread-42] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] FileOutputCommitter: Saved output of task 'attempt_20220612182031286352343316660483_0005_m_000002_0' to hdfs://localhost:9000/haibo.duan/copytest/index.txt/_temporary/0/task_20220612182031286352343316660483_0005_m_000002
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] SparkHadoopMapRedUtil: attempt_20220612182031286352343316660483_0005_m_000002_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 2.0 in stage 1.0 (TID 5)] Executor: Finished task 2.0 in stage 1.0 (TID 5). 1244 bytes result sent to driver
22/06/12 18:20:31 INFO [dispatcher-event-loop-1] TaskSetManager: Starting task 3.0 in stage 1.0 (TID 6) (emr-header-1.cluster-285604, executor driver, partition 3, ANY, 4517 bytes) taskResourceAssignments Map()
22/06/12 18:20:31 INFO [task-result-getter-1] TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 36 ms on emr-header-1.cluster-285604 (executor driver) (3/4)
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] Executor: Running task 3.0 in stage 1.0 (TID 6)
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] HadoopRDD: Input split: hdfs://localhost:9000/haibo.duan/week6/index.txt:54+2
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] FileOutputCommitter: File Output Committer Algorithm version is 1
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] FileOutputCommitter: Saved output of task 'attempt_20220612182031286352343316660483_0005_m_000003_0' to hdfs://localhost:9000/haibo.duan/copytest/index.txt/_temporary/0/task_20220612182031286352343316660483_0005_m_000003
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] SparkHadoopMapRedUtil: attempt_20220612182031286352343316660483_0005_m_000003_0: Committed
22/06/12 18:20:31 INFO [Executor task launch worker for task 3.0 in stage 1.0 (TID 6)] Executor: Finished task 3.0 in stage 1.0 (TID 6). 1115 bytes result sent to driver
22/06/12 18:20:31 INFO [task-result-getter-2] TaskSetManager: Finished task 3.0 in stage 1.0 (TID 6) in 22 ms on emr-header-1.cluster-285604 (executor driver) (4/4)
22/06/12 18:20:31 INFO [task-result-getter-2] TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 0.166 s
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/12 18:20:31 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/06/12 18:20:31 INFO [main] DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 0.177481 s
22/06/12 18:20:31 INFO [main] SparkHadoopWriter: Start to commit write Job job_20220612182031286352343316660483_0005.
22/06/12 18:20:31 INFO [main] SparkHadoopWriter: Write Job job_20220612182031286352343316660483_0005 committed. Elapsed time: 13 ms.
22/06/12 18:20:31 INFO [shutdown-hook-0] SparkContext: Invoking stop() from shutdown hook
22/06/12 18:20:31 INFO [shutdown-hook-0] AbstractConnector: Stopped Spark@1ef173c5{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
22/06/12 18:20:31 INFO [shutdown-hook-0] SparkUI: Stopped Spark web UI at http://emr-header-1.cluster-285604:4041
22/06/12 18:20:31 INFO [dispatcher-event-loop-1] MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/12 18:20:31 INFO [shutdown-hook-0] MemoryStore: MemoryStore cleared
22/06/12 18:20:31 INFO [shutdown-hook-0] BlockManager: BlockManager stopped
22/06/12 18:20:31 INFO [shutdown-hook-0] BlockManagerMaster: BlockManagerMaster stopped
22/06/12 18:20:31 INFO [dispatcher-event-loop-1] OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/12 18:20:31 INFO [shutdown-hook-0] SparkContext: Successfully stopped SparkContext
22/06/12 18:20:31 INFO [shutdown-hook-0] ShutdownHookManager: Shutdown hook called
22/06/12 18:20:31 INFO [shutdown-hook-0] ShutdownHookManager: Deleting directory /tmp/spark-cb1aba69-6de1-43e1-a67e-24b8b6c1ebf8
22/06/12 18:20:31 INFO [shutdown-hook-0] ShutdownHookManager: Deleting directory /tmp/spark-b420eb80-71b9-4006-933f-196ccae23c9d

```
查看目录：
```shell
[student5@emr-header-1 haibo.duan]$ hadoop fs -ls /haibo.duan/copytest/
Found 2 items
drwxr-x--x   - student5 hadoop          0 2022-06-12 18:20 /haibo.duan/copytest/index.csv
drwxr-x--x   - student5 hadoop          0 2022-06-12 18:20 /haibo.duan/copytest/index.txt
[student5@emr-header-1 haibo.duan]$ 
```
