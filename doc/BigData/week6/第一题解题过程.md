第一题解答：
# 1.源码
的project如下：
[spark-demo](spark-demo)
scala代码：
[Index.scala](spark-demo/src/main/java/Index.scala)
```
import org.apache.spark.{SparkConf, SparkContext}

object Index {

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("bingbing").setMaster("local[1]")
    val sc = new SparkContext(sparkConf)
    //调用Spark读取文件API,读取文件内容
//    val url = getClass.getResource("index.txt");
    val wordRDD= sc.textFile("/haibo.duan/week6/index.txt")
      //使用flatMap进行分词后展开
      .flatMap {
        line =>
          //以.做分词需要加转义符
          val array = line.split("\\.", 2)
          val bookName = array(0)
          val result = array(1).replaceAll("\"","")
          result.split(" ").map(word => (bookName, word))
      }

    val kvRDD= wordRDD.map(kv => (kv._2, kv._1)).map((_, 1))
      .reduceByKey((x,y) => x + y)
      .map{case ((k,v),cnt) => (k,(v,cnt))}
      .groupByKey() //只分组不聚合
      .collect()
      .foreach(println)
  }
}
```
pom文件：
[pom.xml](spark-demo/pom.xml)
```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.dhb</groupId>
    <artifactId>spark-demo</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.2.0</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <id>compile-scala</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>test-compile-scala</id>
                        <phase>test-compile</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <scalaVersion>2.12.15</scalaVersion>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass></mainClass>
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.6.0</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <executable>java</executable>
                    <includeProjectDependencies>true</includeProjectDependencies>
                    <includePluginDependencies>false</includePluginDependencies>
                    <classpathScope>compile</classpathScope>
                    <mainClass>cn.spark.study.App</mainClass>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

# 2.spark运行
上传文件到hdfs执行：
1.首先在hdfs上建立目录：
```
#创建目录
[student5@emr-header-1 haibo.duan]$ hadoop fs -mkdir /haibo.duan/week6
[student5@emr-header-1 haibo.duan]$ hadoop fs -ls /haibo.duan
Found 4 items
-rw-r-----   2 student5 hadoop          0 2022-05-13 17:11 /haibo.duan/_SUCCESS
-rw-r-----   2 student5 hadoop        551 2022-05-13 17:11 /haibo.duan/part-r-00000
drwxr-x--x   - student5 hadoop          0 2022-05-29 18:29 /haibo.duan/week4
drwxr-x--x   - student5 hadoop          0 2022-06-12 16:56 /haibo.duan/week6
[student5@emr-header-1 haibo.duan]$ 
```
2.上传文件
```
#上传文件：
[student5@emr-header-1 haibo.duan]$ hadoop fs -put /home/student5/haibo.duan/index.txt /haibo.duan/week6
22/06/12 16:56:55 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[student5@emr-header-1 haibo.duan]$ hadoop fs -ls /haibo.duan/week6
Found 1 items
-rw-r-----   2 student5 hadoop         56 2022-06-12 16:56 /haibo.duan/week6/index.txt
[student5@emr-header-1 haibo.duan]$ 
```
3.通过spark执行结果
```
[student5@emr-header-1 haibo.duan]$ spark-submit --class Index --master local[*] ./spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar 
22/06/12 17:01:22 INFO [main] SparkContext: Running Spark version 3.2.0
22/06/12 17:01:22 INFO [main] ResourceUtils: ==============================================================
22/06/12 17:01:22 INFO [main] ResourceUtils: No custom resources configured for spark.driver.
22/06/12 17:01:22 INFO [main] ResourceUtils: ==============================================================
22/06/12 17:01:22 INFO [main] SparkContext: Submitted application: bingbing
22/06/12 17:01:22 INFO [main] ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2145, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/06/12 17:01:22 INFO [main] ResourceProfile: Limiting resource is cpus at 1 tasks per executor
22/06/12 17:01:22 INFO [main] ResourceProfileManager: Added ResourceProfile id: 0
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
22/06/12 17:01:22 INFO [main] SecurityManager: Changing view acls to: student5,*
22/06/12 17:01:22 INFO [main] SecurityManager: Changing modify acls to: student5
22/06/12 17:01:22 INFO [main] SecurityManager: Changing view acls groups to: 
22/06/12 17:01:22 INFO [main] SecurityManager: Changing modify acls groups to: 
22/06/12 17:01:22 INFO [main] SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(student5, *); groups with view permissions: Set(); users  with modify permissions: Set(student5); groups with modify permissions: Set()
22/06/12 17:01:23 INFO [main] Utils: Successfully started service 'sparkDriver' on port 40981.
22/06/12 17:01:23 INFO [main] SparkEnv: Registering MapOutputTracker
22/06/12 17:01:23 INFO [main] SparkEnv: Registering BlockManagerMaster
22/06/12 17:01:23 INFO [main] BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/12 17:01:23 INFO [main] BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/12 17:01:23 INFO [main] SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/12 17:01:23 INFO [main] DiskBlockManager: Created local directory at /tmp/blockmgr-9b8b85c3-022e-4293-937a-f4df3d02c072
22/06/12 17:01:23 INFO [main] MemoryStore: MemoryStore started with capacity 912.3 MiB
22/06/12 17:01:23 INFO [main] SparkEnv: Registering OutputCommitCoordinator
22/06/12 17:01:23 INFO [main] log: Logging initialized @1778ms to org.sparkproject.jetty.util.log.Slf4jLog
22/06/12 17:01:23 INFO [main] Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_252-b09
22/06/12 17:01:23 INFO [main] Server: Started @1853ms
22/06/12 17:01:23 WARN [main] Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/06/12 17:01:23 INFO [main] AbstractConnector: Started ServerConnector@61526469{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
22/06/12 17:01:23 INFO [main] Utils: Successfully started service 'SparkUI' on port 4041.
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@5f2f577{/jobs,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6a988392{/jobs/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@5b6813df{/jobs/job,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3ebff828{/jobs/job/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@33352f32{/stages,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@1e044120{/stages/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3624da92{/stages/stage,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@2484f433{/stages/stage/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@1255b1d1{/stages/pool,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/pool/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@60bdf15d{/storage,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@51e4ccb3{/storage/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@495083a0{/storage/rdd,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@28a0fd6c{/storage/rdd/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@66629f63{/environment,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@27a5328c{/environment/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6c345c5f{/executors,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@65e61854{/executors/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@4fcee388{/executors/threadDump,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3af17be2{/executors/threadDump/json,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@37f21974{/static,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/api,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job/kill,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/stages/stage/kill,null,AVAILABLE,@Spark}
22/06/12 17:01:23 INFO [main] SparkUI: Bound SparkUI to 0.0.0.0, and started at http://emr-header-1.cluster-285604:4041
22/06/12 17:01:23 INFO [main] SparkContext: Added JAR file:/home/student5/haibo.duan/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://emr-header-1.cluster-285604:40981/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1655024482591
22/06/12 17:01:23 INFO [main] Executor: Starting executor ID driver on host emr-header-1.cluster-285604
22/06/12 17:01:23 INFO [main] Executor: Fetching spark://emr-header-1.cluster-285604:40981/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1655024482591
22/06/12 17:01:23 INFO [main] TransportClientFactory: Successfully created connection to emr-header-1.cluster-285604/192.168.0.197:40981 after 18 ms (0 ms spent in bootstraps)
22/06/12 17:01:23 INFO [main] Utils: Fetching spark://emr-header-1.cluster-285604:40981/jars/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar to /tmp/spark-08de4716-dfc1-490d-84db-e728ed54474a/userFiles-2c229f8f-ba9b-448a-b4f7-51a5d17ece80/fetchFileTemp5704951602863883665.tmp
22/06/12 17:01:24 INFO [main] Executor: Adding file:/tmp/spark-08de4716-dfc1-490d-84db-e728ed54474a/userFiles-2c229f8f-ba9b-448a-b4f7-51a5d17ece80/spark-demo-1.0-SNAPSHOT-jar-with-dependencies.jar to class loader
22/06/12 17:01:24 INFO [main] Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37439.
22/06/12 17:01:24 INFO [main] NettyBlockTransferService: Server created on emr-header-1.cluster-285604:37439
22/06/12 17:01:24 INFO [main] BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/12 17:01:24 INFO [main] BlockManagerMaster: Registering BlockManager BlockManagerId(driver, emr-header-1.cluster-285604, 37439, None)
22/06/12 17:01:24 INFO [dispatcher-BlockManagerMaster] BlockManagerMasterEndpoint: Registering block manager emr-header-1.cluster-285604:37439 with 912.3 MiB RAM, BlockManagerId(driver, emr-header-1.cluster-285604, 37439, None)
22/06/12 17:01:24 INFO [main] BlockManagerMaster: Registered BlockManager BlockManagerId(driver, emr-header-1.cluster-285604, 37439, None)
22/06/12 17:01:24 INFO [main] BlockManager: external shuffle service port = 7337
22/06/12 17:01:24 INFO [main] BlockManager: Initialized BlockManager: BlockManagerId(driver, emr-header-1.cluster-285604, 37439, None)
22/06/12 17:01:24 INFO [main] ContextHandler: Started o.s.j.s.ServletContextHandler@7da10b5b{/metrics/json,null,AVAILABLE,@Spark}
22/06/12 17:01:24 INFO [main] SingleEventLogFileWriter: Logging events to hdfs://emr-header-1.cluster-285604:9000/spark-history/local-1655024483555.inprogress
22/06/12 17:01:24 INFO [Thread-23] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 17:01:25 INFO [main] MemoryStore: Block broadcast_0 stored as values in memory (estimated size 425.8 KiB, free 911.9 MiB)
22/06/12 17:01:25 INFO [main] MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 44.4 KiB, free 911.8 MiB)
22/06/12 17:01:25 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_0_piece0 in memory on emr-header-1.cluster-285604:37439 (size: 44.4 KiB, free: 912.3 MiB)
22/06/12 17:01:25 INFO [main] SparkContext: Created broadcast 0 from textFile at Index.scala:10
22/06/12 17:01:25 INFO [main] GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
22/06/12 17:01:25 INFO [main] LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 97184efe294f64a51a4c5c172cbc22146103da53]
22/06/12 17:01:25 INFO [main] FileInputFormat: Total input files to process : 1
22/06/12 17:01:25 INFO [main] SparkContext: Starting job: collect at Index.scala:23
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Registering RDD 4 (map at Index.scala:21) as input to shuffle 1
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Registering RDD 6 (map at Index.scala:23) as input to shuffle 0
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Got job 0 (collect at Index.scala:23) with 1 output partitions
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Final stage: ResultStage 2 (collect at Index.scala:23)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Missing parents: List(ShuffleMapStage 1)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at map at Index.scala:21), which has no missing parents
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KiB, free 911.8 MiB)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 911.8 MiB)
22/06/12 17:01:25 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_1_piece0 in memory on emr-header-1.cluster-285604:37439 (size: 3.7 KiB, free: 912.3 MiB)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at map at Index.scala:21) (first 15 tasks are for partitions Vector(0))
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
22/06/12 17:01:25 INFO [dispatcher-event-loop-1] TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (emr-header-1.cluster-285604, executor driver, partition 0, ANY, 4524 bytes) taskResourceAssignments Map()
22/06/12 17:01:25 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] Executor: Running task 0.0 in stage 0.0 (TID 0)
22/06/12 17:01:25 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] HadoopRDD: Input split: hdfs://emr-header-1.cluster-285604:9000/haibo.duan/week6/index.txt:0+56
22/06/12 17:01:25 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
22/06/12 17:01:25 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] Executor: Finished task 0.0 in stage 0.0 (TID 0). 1305 bytes result sent to driver
22/06/12 17:01:25 INFO [task-result-getter-0] TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 386 ms on emr-header-1.cluster-285604 (executor driver) (1/1)
22/06/12 17:01:25 INFO [task-result-getter-0] TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: ShuffleMapStage 0 (map at Index.scala:21) finished in 0.487 s
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: looking for newly runnable stages
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: running: Set()
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: failed: Set()
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at map at Index.scala:23), which has no missing parents
22/06/12 17:01:25 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.0 KiB, free 911.8 MiB)
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 911.8 MiB)
22/06/12 17:01:26 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_2_piece0 in memory on emr-header-1.cluster-285604:37439 (size: 3.2 KiB, free: 912.2 MiB)
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1427
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at map at Index.scala:23) (first 15 tasks are for partitions Vector(0))
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
22/06/12 17:01:26 INFO [dispatcher-event-loop-0] TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (emr-header-1.cluster-285604, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Executor: Running task 0.0 in stage 1.0 (TID 1)
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] ShuffleBlockFetcherIterator: Getting 1 (304.0 B) non-empty blocks including 1 (304.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] Executor: Finished task 0.0 in stage 1.0 (TID 1). 1477 bytes result sent to driver
22/06/12 17:01:26 INFO [task-result-getter-1] TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on emr-header-1.cluster-285604 (executor driver) (1/1)
22/06/12 17:01:26 INFO [task-result-getter-1] TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: ShuffleMapStage 1 (map at Index.scala:23) finished in 0.069 s
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: looking for newly runnable stages
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: running: Set()
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: waiting: Set(ResultStage 2)
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: failed: Set()
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting ResultStage 2 (ShuffledRDD[7] at groupByKey at Index.scala:23), which has no missing parents
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KiB, free 911.8 MiB)
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 911.8 MiB)
22/06/12 17:01:26 INFO [dispatcher-BlockManagerMaster] BlockManagerInfo: Added broadcast_3_piece0 in memory on emr-header-1.cluster-285604:37439 (size: 3.5 KiB, free: 912.2 MiB)
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1427
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ShuffledRDD[7] at groupByKey at Index.scala:23) (first 15 tasks are for partitions Vector(0))
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
22/06/12 17:01:26 INFO [dispatcher-event-loop-1] TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (emr-header-1.cluster-285604, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] Executor: Running task 0.0 in stage 2.0 (TID 2)
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] ShuffleBlockFetcherIterator: Getting 1 (304.0 B) non-empty blocks including 1 (304.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
22/06/12 17:01:26 INFO [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] Executor: Finished task 0.0 in stage 2.0 (TID 2). 2032 bytes result sent to driver
22/06/12 17:01:26 INFO [task-result-getter-2] TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on emr-header-1.cluster-285604 (executor driver) (1/1)
22/06/12 17:01:26 INFO [task-result-getter-2] TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: ResultStage 2 (collect at Index.scala:23) finished in 0.025 s
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/06/12 17:01:26 INFO [dag-scheduler-event-loop] TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
22/06/12 17:01:26 INFO [main] DAGScheduler: Job 0 finished: collect at Index.scala:23, took 0.666679 s
(banana,CompactBuffer((2,1)))
(it,CompactBuffer((1,1), (0,2), (2,1)))
(is,CompactBuffer((2,1), (1,1), (0,2)))
(a,CompactBuffer((2,1)))
(what,CompactBuffer((0,1), (1,1)))
22/06/12 17:01:26 INFO [shutdown-hook-0] SparkContext: Invoking stop() from shutdown hook
22/06/12 17:01:26 INFO [shutdown-hook-0] AbstractConnector: Stopped Spark@61526469{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
22/06/12 17:01:26 INFO [shutdown-hook-0] SparkUI: Stopped Spark web UI at http://emr-header-1.cluster-285604:4041
22/06/12 17:01:26 INFO [dispatcher-event-loop-0] MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/06/12 17:01:26 INFO [shutdown-hook-0] MemoryStore: MemoryStore cleared
22/06/12 17:01:26 INFO [shutdown-hook-0] BlockManager: BlockManager stopped
22/06/12 17:01:26 INFO [shutdown-hook-0] BlockManagerMaster: BlockManagerMaster stopped
22/06/12 17:01:26 INFO [dispatcher-event-loop-0] OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/06/12 17:01:26 INFO [shutdown-hook-0] SparkContext: Successfully stopped SparkContext
22/06/12 17:01:26 INFO [shutdown-hook-0] ShutdownHookManager: Shutdown hook called
22/06/12 17:01:26 INFO [shutdown-hook-0] ShutdownHookManager: Deleting directory /tmp/spark-84359702-2f1f-498a-ba71-4061b438c2e2
22/06/12 17:01:26 INFO [shutdown-hook-0] ShutdownHookManager: Deleting directory /tmp/spark-08de4716-dfc1-490d-84db-e728ed54474a
[student5@emr-header-1 haibo.duan]$ 
```
注意这里的输出结果：
```
(banana,CompactBuffer((2,1)))
(it,CompactBuffer((1,1), (0,2), (2,1)))
(is,CompactBuffer((2,1), (1,1), (0,2)))
(a,CompactBuffer((2,1)))
(what,CompactBuffer((0,1), (1,1)))
```
这个结果即为所得到的输出。

#3.DataFrame
将文件转换为csv:
```csv
id,c1,c2,c3,c4,c5
0,it,is,what,it,is
1,what,is,it
2,it,is,a,banana
```
上传到hdfs:
```shell
[student5@emr-header-1 haibo.duan]$ vim index.csv
[student5@emr-header-1 haibo.duan]$ hadoop fs -put /home/student5/haibo.duan/index.csv /haibo.duan/week6/
22/06/12 17:11:49 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
```

在spark-shell中运行如下代码：
```
import org.apache.spark.sql.DataFrame
//因为Spark加载的每一列数据无视含义,统一视为String类型,下面转换为int
import org.apache.spark.sql.types.{StringType,IntegerType,StructType,StructField}
val schema:StructType = StructType(Array(StructField("id",IntegerType),StructField("c1",StringType),StructField("c2",StringType),StructField("c3",StringType),StructField("c4",StringType),StructField("c5",StringType)))
//创建DataFrame
val csvdf:DataFrame = spark.read.format("csv").schema(schema).option("header",true).option("seq",",").load("/haibo.duan/week6/index.csv")
//创建DataFrame
csvdf.createTempView("index")
//定义查询语句,下面行转列就用union all了
val query:String = "select id,c1,count(1) as num from (select id,c1 from index union all select id,c2 from index union all select id,c3 from index union all select id,c4 from index union all select id,c5 from index) t group by id,c1"
//使用spark sql
val result:DataFrame = spark.sql(query)
result.show
```
运行过程及输出：
```shell
[student5@emr-header-1 haibo.duan]$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
22/06/12 17:13:40 WARN [main] Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/06/12 17:13:41 WARN [main] Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark context Web UI available at http://emr-header-1.cluster-285604:4041
Spark context available as 'sc' (master = yarn, app id = application_1645699879292_3693).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_252)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.DataFrame

scala> import org.apache.spark.sql.types.{StringType,IntegerType,StructType,StructField}
import org.apache.spark.sql.types.{StringType, IntegerType, StructType, StructField}

scala> val schema:StructType = StructType(Array(StructField("id",IntegerType),StructField("c1",StringType),StructField("c2",StringType),StructField("c3",StringType),StructField("c4",StringType),StructField("c5",StringType)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(c1,StringType,true), StructField(c2,StringType,true), StructField(c3,StringType,true), StructField(c4,StringType,true), StructField(c5,StringType,true))

scala> val csvdf:DataFrame = spark.read.format("csv").schema(schema).option("header",true).option("seq",",").load("/haibo.duan/week6/index.csv")
csvdf: org.apache.spark.sql.DataFrame = [id: int, c1: string ... 4 more fields]

scala> csvdf.createTempView("index")

scala> val query:String = "select id,c1,count(1) as num from (select id,c1 from index union all select id,c2 from index union all select id,c3 from index union all select id,c4 from index union all select id,c5 from index) t group by id,c1"
query: String = select id,c1,count(1) as num from (select id,c1 from index union all select id,c2 from index union all select id,c3 from index union all select id,c4 from index union all select id,c5 from index) t group by id,c1

scala> val result:DataFrame = spark.sql(query)
result: org.apache.spark.sql.DataFrame = [id: int, c1: string ... 1 more field]

scala> result.show
+---+------+---+                                                                
| id|    c1|num|
+---+------+---+
|  1|    is|  1|
|  2|    is|  1|
|  0|    is|  2|
|  2|     a|  1|
|  1|    it|  1|
|  0|  what|  1|
|  1|  what|  1|
|  0|    it|  2|
|  2|    it|  1|
|  2|banana|  1|
|  1|  null|  2|
|  2|  null|  1|
+---+------+---+

scala> 
```