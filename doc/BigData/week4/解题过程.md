# 1. 数据准备
新建目录：
```
hadoop fs -mkdir /haibo.duan/week4
```
从/data/hive copy一份数据到自己的hdfs文件夹下：
```
hadoop fs -cp /data/hive/users /haibo.duan/week4
hadoop fs -cp /data/hive/movies  /haibo.duan/week4
hadoop fs -cp /data/hive/ratings /haibo.duan/week4
```
建表：
为了避免与其他人冲突，表名加后缀dhb
t_movie_dhb:
```sql
-- 建t_movie_dhb表
CREATE TABLE `t_movie_dhb`(
    `movie_id` bigint COMMENT '电影id',
    `movie_name` string COMMENT '电影名字',
    `movie_type` string COMMENT '电影类型')
ROW FORMAT SERDE 
    'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
WITH SERDEPROPERTIES ( 
    'field.delim'='::') -- 按::进行分隔，如果数据本身是别的分隔符，按具体情况选择，例如：\t
STORED AS INPUTFORMAT 
    'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 
    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
    '/haibo.duan/week4/movies' -- hdfs文件路径
TBLPROPERTIES (
    'bucketing_version'='2',
    'transient_lastDdlTime'='1648533877');
```
t_rating_dhb
```sql
-- 建t_rating_dhb表
CREATE TABLE `t_rating_dhb`(
    `user_id` int COMMENT '用户id',
    `movie_id` bigint COMMENT '电影id',
    `rate` int COMMENT '评分',
    `times` string COMMENT '评分时间')
ROW FORMAT SERDE
    'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
WITH SERDEPROPERTIES ( 
    'field.delim'='::') 
STORED AS INPUTFORMAT 
    'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
    '/haibo.duan/week4/ratings' -- hdfs文件路径
TBLPROPERTIES (
    'bucketing_version'='2',
    'transient_lastDdlTime'='1648534400');
```
t_user_dhb:
```sql
-- 建t_user_dhb表
CREATE TABLE `t_user_dhb`(
    `user_id` int COMMENT '用户id',
    `sex` string COMMENT '性别', 
    `age` int COMMENT '年龄',
    `occupation` string COMMENT '职业',
    `zip_code` bigint COMMENT '邮编')
ROW FORMAT SERDE 
    'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
WITH SERDEPROPERTIES (
    'field.delim'='::') 
STORED AS INPUTFORMAT
    'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
    '/haibo.duan/week4/users'  -- hdfs文件路径
TBLPROPERTIES (
    'bucketing_version'='2',
    'transient_lastDdlTime'='1648534260');
```

上述sql存到sql文件，hive执行建表语句：
```
hive -f ./t_movie.sql;
hive -f ./t_rating.sql;
hive -f ./t_user.sql;
```

# 2.第一题
第一题答案：
```sql
select  c.age,avg(b.rate) rate_avg 
from t_movie_dhb a 
    join t_rating_dhb b on a.movie_id = b.movie_id 
    join t_user_dhb c on c.user_id = b.user_id 
where a.movie_id = 2116 
group by c.age 
order by c.age;
```
第一题执行效果：
![第一题执行效果.png](第一题执行效果.png)

